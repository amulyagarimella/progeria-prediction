{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "progeria = pd.read_csv(r\"C:\\Users\\amulya\\Documents\\progeria-prediction\\230705_progeria_c2\\IdentifyPrimaryObjects.csv\")\n",
    "normal = pd.read_csv(r\"C:\\Users\\amulya\\Documents\\progeria-prediction\\230705_normal_c2\\IdentifyPrimaryObjects.csv\")\n",
    "cols_to_drop = [\"ImageNumber\", \"ObjectNumber\", \"Number_Object_Number\"]\n",
    "progeria_features = progeria.drop(columns=cols_to_drop)\n",
    "progeria_features = progeria_features.assign(label=1) \n",
    "normal_features = normal.drop(columns=cols_to_drop)\n",
    "normal_features = progeria_features.assign(label=0) \n",
    "full_data = pd.concat([progeria_features, normal_features],axis=0)\n",
    "full_data = full_data.replace([np.inf, -np.inf], np.nan)\n",
    "full_data = full_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellFeaturesDataset(Dataset):\n",
    "    def __init__(self, fulldata, transform=None, target_transform=None):\n",
    "        self.labels = fulldata.loc[:,'label'].values\n",
    "        self.data = fulldata.loc[:,~fulldata.columns.isin(['label'])].values\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        item = self.data[idx,:]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            item = self.transform(item)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return item.float(), label.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_and_scale (x):\n",
    "    m = x.mean(0)\n",
    "    s = x.std(0)\n",
    "    x -= m\n",
    "    x /= s\n",
    "    # torch.allclose(x, torch.from_numpy(arr_norm))\n",
    "    return torch.as_tensor(x)\n",
    "\n",
    "\n",
    "t = tensor_and_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_dataset = CellFeaturesDataset(full_data, transform=torch.as_tensor, target_transform=torch.as_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cells, train_cells = torch.utils.data.random_split(cell_dataset, [.3, .7])\n",
    "train_dataloader = DataLoader(train_cells, batch_size=10, shuffle=True)\n",
    "test_dataloader = DataLoader(test_cells, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features=58, out_features=40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=40, out_features=40),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=40, out_features=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # do stuff here\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        prediction = torch.special.expit(logits)\n",
    "        return logits, prediction\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "learning_rate = 1e-8\n",
    "batch_size = 10\n",
    "epochs = 15\n",
    "loss_fn = nn.NLLLoss()\n",
    "model = NeuralNetwork()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    model.float()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Compute prediction error\n",
    "        pred = model(X)[0]\n",
    "        loss = loss_fn(pred, y.long())\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        #torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)[0]\n",
    "            test_loss += loss_fn(pred, y.long()).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 48.145546  [   10/ 2948]\n",
      "loss: 41.183308  [  110/ 2948]\n",
      "loss: 39.526501  [  210/ 2948]\n",
      "loss: 49.551472  [  310/ 2948]\n",
      "loss: 59.150280  [  410/ 2948]\n",
      "loss: 34.064510  [  510/ 2948]\n",
      "loss: 44.669746  [  610/ 2948]\n",
      "loss: 51.137062  [  710/ 2948]\n",
      "loss: 28.378424  [  810/ 2948]\n",
      "loss: 38.881012  [  910/ 2948]\n",
      "loss: 40.675896  [ 1010/ 2948]\n",
      "loss: 37.966351  [ 1110/ 2948]\n",
      "loss: 38.425873  [ 1210/ 2948]\n",
      "loss: 43.747559  [ 1310/ 2948]\n",
      "loss: 54.758308  [ 1410/ 2948]\n",
      "loss: 53.821602  [ 1510/ 2948]\n",
      "loss: 42.215294  [ 1610/ 2948]\n",
      "loss: 40.812523  [ 1710/ 2948]\n",
      "loss: 42.894314  [ 1810/ 2948]\n",
      "loss: 36.340618  [ 1910/ 2948]\n",
      "loss: 48.006531  [ 2010/ 2948]\n",
      "loss: 42.000820  [ 2110/ 2948]\n",
      "loss: 44.050671  [ 2210/ 2948]\n",
      "loss: 35.181725  [ 2310/ 2948]\n",
      "loss: 41.206448  [ 2410/ 2948]\n",
      "loss: 47.509750  [ 2510/ 2948]\n",
      "loss: 42.847424  [ 2610/ 2948]\n",
      "loss: 36.814625  [ 2710/ 2948]\n",
      "loss: 36.178825  [ 2810/ 2948]\n",
      "loss: 41.548977  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 59.033449 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 45.476360  [   10/ 2948]\n",
      "loss: 35.073971  [  110/ 2948]\n",
      "loss: 55.185966  [  210/ 2948]\n",
      "loss: 60.873085  [  310/ 2948]\n",
      "loss: 43.115517  [  410/ 2948]\n",
      "loss: 267.456848  [  510/ 2948]\n",
      "loss: 54.988850  [  610/ 2948]\n",
      "loss: 50.251411  [  710/ 2948]\n",
      "loss: 39.364765  [  810/ 2948]\n",
      "loss: 41.650070  [  910/ 2948]\n",
      "loss: 39.824017  [ 1010/ 2948]\n",
      "loss: 93.381119  [ 1110/ 2948]\n",
      "loss: 46.207058  [ 1210/ 2948]\n",
      "loss: 49.572292  [ 1310/ 2948]\n",
      "loss: 42.023518  [ 1410/ 2948]\n",
      "loss: 49.541710  [ 1510/ 2948]\n",
      "loss: 45.362549  [ 1610/ 2948]\n",
      "loss: 43.336723  [ 1710/ 2948]\n",
      "loss: 36.505081  [ 1810/ 2948]\n",
      "loss: 37.013393  [ 1910/ 2948]\n",
      "loss: 36.624733  [ 2010/ 2948]\n",
      "loss: 43.129436  [ 2110/ 2948]\n",
      "loss: 46.384682  [ 2210/ 2948]\n",
      "loss: 33.458244  [ 2310/ 2948]\n",
      "loss: 124.613968  [ 2410/ 2948]\n",
      "loss: 53.967033  [ 2510/ 2948]\n",
      "loss: 46.291668  [ 2610/ 2948]\n",
      "loss: 45.448906  [ 2710/ 2948]\n",
      "loss: 35.897861  [ 2810/ 2948]\n",
      "loss: 53.434223  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 57.682730 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 36.935474  [   10/ 2948]\n",
      "loss: 31.576061  [  110/ 2948]\n",
      "loss: 53.186451  [  210/ 2948]\n",
      "loss: 129.395309  [  310/ 2948]\n",
      "loss: 56.704540  [  410/ 2948]\n",
      "loss: 52.188774  [  510/ 2948]\n",
      "loss: 39.423332  [  610/ 2948]\n",
      "loss: 42.599236  [  710/ 2948]\n",
      "loss: 40.923210  [  810/ 2948]\n",
      "loss: 42.809032  [  910/ 2948]\n",
      "loss: 41.975792  [ 1010/ 2948]\n",
      "loss: 50.648731  [ 1110/ 2948]\n",
      "loss: 34.941471  [ 1210/ 2948]\n",
      "loss: 43.764484  [ 1310/ 2948]\n",
      "loss: 48.881725  [ 1410/ 2948]\n",
      "loss: 35.193008  [ 1510/ 2948]\n",
      "loss: 41.036537  [ 1610/ 2948]\n",
      "loss: 42.628723  [ 1710/ 2948]\n",
      "loss: 56.246624  [ 1810/ 2948]\n",
      "loss: 40.974098  [ 1910/ 2948]\n",
      "loss: 39.267891  [ 2010/ 2948]\n",
      "loss: 416.437988  [ 2110/ 2948]\n",
      "loss: 43.874073  [ 2210/ 2948]\n",
      "loss: 39.496967  [ 2310/ 2948]\n",
      "loss: 42.324917  [ 2410/ 2948]\n",
      "loss: 52.163124  [ 2510/ 2948]\n",
      "loss: 36.174637  [ 2610/ 2948]\n",
      "loss: 35.438011  [ 2710/ 2948]\n",
      "loss: 199.903259  [ 2810/ 2948]\n",
      "loss: 40.229256  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 56.461653 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 42.659382  [   10/ 2948]\n",
      "loss: 32.441917  [  110/ 2948]\n",
      "loss: 41.252670  [  210/ 2948]\n",
      "loss: 39.541149  [  310/ 2948]\n",
      "loss: 30.794392  [  410/ 2948]\n",
      "loss: 37.277439  [  510/ 2948]\n",
      "loss: 46.081745  [  610/ 2948]\n",
      "loss: 42.053894  [  710/ 2948]\n",
      "loss: 33.869530  [  810/ 2948]\n",
      "loss: 54.190346  [  910/ 2948]\n",
      "loss: 254.659622  [ 1010/ 2948]\n",
      "loss: 54.503914  [ 1110/ 2948]\n",
      "loss: 48.564602  [ 1210/ 2948]\n",
      "loss: 39.576336  [ 1310/ 2948]\n",
      "loss: 42.669720  [ 1410/ 2948]\n",
      "loss: 33.665512  [ 1510/ 2948]\n",
      "loss: 34.256180  [ 1610/ 2948]\n",
      "loss: 37.725887  [ 1710/ 2948]\n",
      "loss: 51.760994  [ 1810/ 2948]\n",
      "loss: 42.482101  [ 1910/ 2948]\n",
      "loss: 47.563492  [ 2010/ 2948]\n",
      "loss: 35.188229  [ 2110/ 2948]\n",
      "loss: 47.400581  [ 2210/ 2948]\n",
      "loss: 40.979561  [ 2310/ 2948]\n",
      "loss: 44.924240  [ 2410/ 2948]\n",
      "loss: 28.920715  [ 2510/ 2948]\n",
      "loss: 56.211353  [ 2610/ 2948]\n",
      "loss: 44.844536  [ 2710/ 2948]\n",
      "loss: 39.806458  [ 2810/ 2948]\n",
      "loss: 36.309273  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 55.272598 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 30.260397  [   10/ 2948]\n",
      "loss: 46.081944  [  110/ 2948]\n",
      "loss: 190.694122  [  210/ 2948]\n",
      "loss: 37.800781  [  310/ 2948]\n",
      "loss: 34.777733  [  410/ 2948]\n",
      "loss: 36.951214  [  510/ 2948]\n",
      "loss: 36.279575  [  610/ 2948]\n",
      "loss: 46.322895  [  710/ 2948]\n",
      "loss: 42.895523  [  810/ 2948]\n",
      "loss: 43.776394  [  910/ 2948]\n",
      "loss: 46.466797  [ 1010/ 2948]\n",
      "loss: 44.033978  [ 1110/ 2948]\n",
      "loss: 43.625847  [ 1210/ 2948]\n",
      "loss: 37.799080  [ 1310/ 2948]\n",
      "loss: 45.006359  [ 1410/ 2948]\n",
      "loss: 242.075439  [ 1510/ 2948]\n",
      "loss: 42.280857  [ 1610/ 2948]\n",
      "loss: 44.148464  [ 1710/ 2948]\n",
      "loss: 41.635315  [ 1810/ 2948]\n",
      "loss: 112.188026  [ 1910/ 2948]\n",
      "loss: 35.135277  [ 2010/ 2948]\n",
      "loss: 30.787556  [ 2110/ 2948]\n",
      "loss: 38.825001  [ 2210/ 2948]\n",
      "loss: 41.941032  [ 2310/ 2948]\n",
      "loss: 35.921642  [ 2410/ 2948]\n",
      "loss: 39.028992  [ 2510/ 2948]\n",
      "loss: 37.905956  [ 2610/ 2948]\n",
      "loss: 62.262077  [ 2710/ 2948]\n",
      "loss: 41.455910  [ 2810/ 2948]\n",
      "loss: 36.114983  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 54.140199 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 33.829220  [   10/ 2948]\n",
      "loss: 172.186249  [  110/ 2948]\n",
      "loss: 46.244175  [  210/ 2948]\n",
      "loss: 40.339268  [  310/ 2948]\n",
      "loss: 33.994011  [  410/ 2948]\n",
      "loss: 40.429569  [  510/ 2948]\n",
      "loss: 43.797707  [  610/ 2948]\n",
      "loss: 35.510666  [  710/ 2948]\n",
      "loss: 51.418762  [  810/ 2948]\n",
      "loss: 37.477962  [  910/ 2948]\n",
      "loss: 42.383980  [ 1010/ 2948]\n",
      "loss: 121.032791  [ 1110/ 2948]\n",
      "loss: 42.913979  [ 1210/ 2948]\n",
      "loss: 49.040279  [ 1310/ 2948]\n",
      "loss: 38.294952  [ 1410/ 2948]\n",
      "loss: 45.033115  [ 1510/ 2948]\n",
      "loss: 41.124123  [ 1610/ 2948]\n",
      "loss: 35.438869  [ 1710/ 2948]\n",
      "loss: 121.120987  [ 1810/ 2948]\n",
      "loss: 40.352421  [ 1910/ 2948]\n",
      "loss: 442.215820  [ 2010/ 2948]\n",
      "loss: 43.911137  [ 2110/ 2948]\n",
      "loss: 42.731354  [ 2210/ 2948]\n",
      "loss: 36.698582  [ 2310/ 2948]\n",
      "loss: 53.575096  [ 2410/ 2948]\n",
      "loss: 51.110649  [ 2510/ 2948]\n",
      "loss: 41.502625  [ 2610/ 2948]\n",
      "loss: 37.247047  [ 2710/ 2948]\n",
      "loss: 44.797577  [ 2810/ 2948]\n",
      "loss: 32.704201  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 53.059329 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 39.716255  [   10/ 2948]\n",
      "loss: 35.688732  [  110/ 2948]\n",
      "loss: 42.677204  [  210/ 2948]\n",
      "loss: 40.671806  [  310/ 2948]\n",
      "loss: 39.633812  [  410/ 2948]\n",
      "loss: 32.572193  [  510/ 2948]\n",
      "loss: 48.549248  [  610/ 2948]\n",
      "loss: 37.406792  [  710/ 2948]\n",
      "loss: 37.837357  [  810/ 2948]\n",
      "loss: 45.508354  [  910/ 2948]\n",
      "loss: 38.217888  [ 1010/ 2948]\n",
      "loss: 32.631367  [ 1110/ 2948]\n",
      "loss: 37.755920  [ 1210/ 2948]\n",
      "loss: 36.034763  [ 1310/ 2948]\n",
      "loss: 32.665184  [ 1410/ 2948]\n",
      "loss: 38.234978  [ 1510/ 2948]\n",
      "loss: 33.796989  [ 1610/ 2948]\n",
      "loss: 34.045975  [ 1710/ 2948]\n",
      "loss: 36.877270  [ 1810/ 2948]\n",
      "loss: 34.156967  [ 1910/ 2948]\n",
      "loss: 51.268024  [ 2010/ 2948]\n",
      "loss: 36.423256  [ 2110/ 2948]\n",
      "loss: 33.812313  [ 2210/ 2948]\n",
      "loss: 40.350826  [ 2310/ 2948]\n",
      "loss: 43.263474  [ 2410/ 2948]\n",
      "loss: 40.574707  [ 2510/ 2948]\n",
      "loss: 273.388947  [ 2610/ 2948]\n",
      "loss: 36.610374  [ 2710/ 2948]\n",
      "loss: 40.890461  [ 2810/ 2948]\n",
      "loss: 33.261608  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 51.758142 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 36.436947  [   10/ 2948]\n",
      "loss: 35.061844  [  110/ 2948]\n",
      "loss: 36.879711  [  210/ 2948]\n",
      "loss: 44.524315  [  310/ 2948]\n",
      "loss: 186.598633  [  410/ 2948]\n",
      "loss: 45.140526  [  510/ 2948]\n",
      "loss: 40.250782  [  610/ 2948]\n",
      "loss: 42.767120  [  710/ 2948]\n",
      "loss: 46.332062  [  810/ 2948]\n",
      "loss: 43.251434  [  910/ 2948]\n",
      "loss: 52.474854  [ 1010/ 2948]\n",
      "loss: 47.046120  [ 1110/ 2948]\n",
      "loss: 30.662531  [ 1210/ 2948]\n",
      "loss: 226.834930  [ 1310/ 2948]\n",
      "loss: 35.998634  [ 1410/ 2948]\n",
      "loss: 229.573288  [ 1510/ 2948]\n",
      "loss: 36.296478  [ 1610/ 2948]\n",
      "loss: 39.185032  [ 1710/ 2948]\n",
      "loss: 41.079338  [ 1810/ 2948]\n",
      "loss: 39.993206  [ 1910/ 2948]\n",
      "loss: 44.786308  [ 2010/ 2948]\n",
      "loss: 36.528328  [ 2110/ 2948]\n",
      "loss: 31.458706  [ 2210/ 2948]\n",
      "loss: 27.509832  [ 2310/ 2948]\n",
      "loss: 33.662525  [ 2410/ 2948]\n",
      "loss: 46.907951  [ 2510/ 2948]\n",
      "loss: 35.396927  [ 2610/ 2948]\n",
      "loss: 37.406059  [ 2710/ 2948]\n",
      "loss: 40.092415  [ 2810/ 2948]\n",
      "loss: 39.636147  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 50.463667 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 34.440418  [   10/ 2948]\n",
      "loss: 230.980057  [  110/ 2948]\n",
      "loss: 28.999298  [  210/ 2948]\n",
      "loss: 35.591022  [  310/ 2948]\n",
      "loss: 44.903152  [  410/ 2948]\n",
      "loss: 37.065144  [  510/ 2948]\n",
      "loss: 34.634819  [  610/ 2948]\n",
      "loss: 40.968948  [  710/ 2948]\n",
      "loss: 34.434666  [  810/ 2948]\n",
      "loss: 31.319920  [  910/ 2948]\n",
      "loss: 30.970297  [ 1010/ 2948]\n",
      "loss: 35.859337  [ 1110/ 2948]\n",
      "loss: 26.252905  [ 1210/ 2948]\n",
      "loss: 34.055275  [ 1310/ 2948]\n",
      "loss: 37.110901  [ 1410/ 2948]\n",
      "loss: 30.132711  [ 1510/ 2948]\n",
      "loss: 34.601143  [ 1610/ 2948]\n",
      "loss: 28.078955  [ 1710/ 2948]\n",
      "loss: 47.827934  [ 1810/ 2948]\n",
      "loss: 34.205166  [ 1910/ 2948]\n",
      "loss: 31.994379  [ 2010/ 2948]\n",
      "loss: 29.798376  [ 2110/ 2948]\n",
      "loss: 37.417221  [ 2210/ 2948]\n",
      "loss: 31.274271  [ 2310/ 2948]\n",
      "loss: 34.618484  [ 2410/ 2948]\n",
      "loss: 33.966606  [ 2510/ 2948]\n",
      "loss: 34.823784  [ 2610/ 2948]\n",
      "loss: 42.051323  [ 2710/ 2948]\n",
      "loss: 37.491772  [ 2810/ 2948]\n",
      "loss: 34.774117  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 49.339647 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 32.512707  [   10/ 2948]\n",
      "loss: 39.804775  [  110/ 2948]\n",
      "loss: 38.191101  [  210/ 2948]\n",
      "loss: 37.241310  [  310/ 2948]\n",
      "loss: 27.246023  [  410/ 2948]\n",
      "loss: 36.112770  [  510/ 2948]\n",
      "loss: 38.637623  [  610/ 2948]\n",
      "loss: 33.933140  [  710/ 2948]\n",
      "loss: 35.744347  [  810/ 2948]\n",
      "loss: 39.530617  [  910/ 2948]\n",
      "loss: 34.609444  [ 1010/ 2948]\n",
      "loss: 26.341471  [ 1110/ 2948]\n",
      "loss: 34.577995  [ 1210/ 2948]\n",
      "loss: 99.212959  [ 1310/ 2948]\n",
      "loss: 33.490723  [ 1410/ 2948]\n",
      "loss: 43.234810  [ 1510/ 2948]\n",
      "loss: 37.244606  [ 1610/ 2948]\n",
      "loss: 59.326874  [ 1710/ 2948]\n",
      "loss: 24.113283  [ 1810/ 2948]\n",
      "loss: 29.974049  [ 1910/ 2948]\n",
      "loss: 37.419163  [ 2010/ 2948]\n",
      "loss: 202.400055  [ 2110/ 2948]\n",
      "loss: 36.030735  [ 2210/ 2948]\n",
      "loss: 29.631714  [ 2310/ 2948]\n",
      "loss: 31.927118  [ 2410/ 2948]\n",
      "loss: 34.849575  [ 2510/ 2948]\n",
      "loss: 48.455254  [ 2610/ 2948]\n",
      "loss: 34.443855  [ 2710/ 2948]\n",
      "loss: 48.074100  [ 2810/ 2948]\n",
      "loss: 38.425045  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 48.237435 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 38.653625  [   10/ 2948]\n",
      "loss: 40.860470  [  110/ 2948]\n",
      "loss: 30.265209  [  210/ 2948]\n",
      "loss: 27.694906  [  310/ 2948]\n",
      "loss: 31.442469  [  410/ 2948]\n",
      "loss: 29.282928  [  510/ 2948]\n",
      "loss: 33.898487  [  610/ 2948]\n",
      "loss: 26.489710  [  710/ 2948]\n",
      "loss: 32.413261  [  810/ 2948]\n",
      "loss: 44.411160  [  910/ 2948]\n",
      "loss: 27.728888  [ 1010/ 2948]\n",
      "loss: 34.567917  [ 1110/ 2948]\n",
      "loss: 32.731735  [ 1210/ 2948]\n",
      "loss: 43.069939  [ 1310/ 2948]\n",
      "loss: 216.373016  [ 1410/ 2948]\n",
      "loss: 33.174965  [ 1510/ 2948]\n",
      "loss: 218.098663  [ 1610/ 2948]\n",
      "loss: 41.837746  [ 1710/ 2948]\n",
      "loss: 33.283642  [ 1810/ 2948]\n",
      "loss: 36.175392  [ 1910/ 2948]\n",
      "loss: 34.925022  [ 2010/ 2948]\n",
      "loss: 37.860497  [ 2110/ 2948]\n",
      "loss: 405.788879  [ 2210/ 2948]\n",
      "loss: 37.281639  [ 2310/ 2948]\n",
      "loss: 33.915894  [ 2410/ 2948]\n",
      "loss: 31.970226  [ 2510/ 2948]\n",
      "loss: 31.846252  [ 2610/ 2948]\n",
      "loss: 48.741501  [ 2710/ 2948]\n",
      "loss: 28.973110  [ 2810/ 2948]\n",
      "loss: 32.017715  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 47.053929 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 39.425438  [   10/ 2948]\n",
      "loss: 40.979401  [  110/ 2948]\n",
      "loss: 35.302299  [  210/ 2948]\n",
      "loss: 39.579575  [  310/ 2948]\n",
      "loss: 30.392843  [  410/ 2948]\n",
      "loss: 23.037920  [  510/ 2948]\n",
      "loss: 27.438251  [  610/ 2948]\n",
      "loss: 30.633368  [  710/ 2948]\n",
      "loss: 27.117222  [  810/ 2948]\n",
      "loss: 39.944298  [  910/ 2948]\n",
      "loss: 90.287453  [ 1010/ 2948]\n",
      "loss: 165.993866  [ 1110/ 2948]\n",
      "loss: 51.163338  [ 1210/ 2948]\n",
      "loss: 35.533531  [ 1310/ 2948]\n",
      "loss: 31.588053  [ 1410/ 2948]\n",
      "loss: 23.530882  [ 1510/ 2948]\n",
      "loss: 30.791599  [ 1610/ 2948]\n",
      "loss: 39.178467  [ 1710/ 2948]\n",
      "loss: 34.244637  [ 1810/ 2948]\n",
      "loss: 34.116714  [ 1910/ 2948]\n",
      "loss: 25.932865  [ 2010/ 2948]\n",
      "loss: 35.597679  [ 2110/ 2948]\n",
      "loss: 45.917152  [ 2210/ 2948]\n",
      "loss: 29.922455  [ 2310/ 2948]\n",
      "loss: 29.765472  [ 2410/ 2948]\n",
      "loss: 24.300344  [ 2510/ 2948]\n",
      "loss: 51.567482  [ 2610/ 2948]\n",
      "loss: 35.374737  [ 2710/ 2948]\n",
      "loss: 35.333427  [ 2810/ 2948]\n",
      "loss: 29.712301  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 45.934881 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 33.948936  [   10/ 2948]\n",
      "loss: 47.700184  [  110/ 2948]\n",
      "loss: 37.290260  [  210/ 2948]\n",
      "loss: 36.773327  [  310/ 2948]\n",
      "loss: 32.852745  [  410/ 2948]\n",
      "loss: 34.579590  [  510/ 2948]\n",
      "loss: 27.100430  [  610/ 2948]\n",
      "loss: 42.601864  [  710/ 2948]\n",
      "loss: 26.323355  [  810/ 2948]\n",
      "loss: 26.659338  [  910/ 2948]\n",
      "loss: 30.401388  [ 1010/ 2948]\n",
      "loss: 29.905720  [ 1110/ 2948]\n",
      "loss: 35.260803  [ 1210/ 2948]\n",
      "loss: 38.718410  [ 1310/ 2948]\n",
      "loss: 40.014671  [ 1410/ 2948]\n",
      "loss: 35.480633  [ 1510/ 2948]\n",
      "loss: 27.874584  [ 1610/ 2948]\n",
      "loss: 31.958109  [ 1710/ 2948]\n",
      "loss: 28.360012  [ 1810/ 2948]\n",
      "loss: 32.147720  [ 1910/ 2948]\n",
      "loss: 31.039318  [ 2010/ 2948]\n",
      "loss: 31.516489  [ 2110/ 2948]\n",
      "loss: 32.531841  [ 2210/ 2948]\n",
      "loss: 96.055893  [ 2310/ 2948]\n",
      "loss: 80.904892  [ 2410/ 2948]\n",
      "loss: 37.645744  [ 2510/ 2948]\n",
      "loss: 39.079777  [ 2610/ 2948]\n",
      "loss: 26.671200  [ 2710/ 2948]\n",
      "loss: 40.762299  [ 2810/ 2948]\n",
      "loss: 34.820377  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 44.904534 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 32.585155  [   10/ 2948]\n",
      "loss: 30.880758  [  110/ 2948]\n",
      "loss: 25.042540  [  210/ 2948]\n",
      "loss: 27.251019  [  310/ 2948]\n",
      "loss: 21.529263  [  410/ 2948]\n",
      "loss: 37.876629  [  510/ 2948]\n",
      "loss: 37.086906  [  610/ 2948]\n",
      "loss: 35.947083  [  710/ 2948]\n",
      "loss: 36.528168  [  810/ 2948]\n",
      "loss: 32.762718  [  910/ 2948]\n",
      "loss: 35.954548  [ 1010/ 2948]\n",
      "loss: 176.224182  [ 1110/ 2948]\n",
      "loss: 35.581875  [ 1210/ 2948]\n",
      "loss: 34.474205  [ 1310/ 2948]\n",
      "loss: 26.723392  [ 1410/ 2948]\n",
      "loss: 23.949400  [ 1510/ 2948]\n",
      "loss: 31.728083  [ 1610/ 2948]\n",
      "loss: 29.012884  [ 1710/ 2948]\n",
      "loss: 37.060955  [ 1810/ 2948]\n",
      "loss: 31.525394  [ 1910/ 2948]\n",
      "loss: 36.810089  [ 2010/ 2948]\n",
      "loss: 39.409138  [ 2110/ 2948]\n",
      "loss: 35.075089  [ 2210/ 2948]\n",
      "loss: 36.249660  [ 2310/ 2948]\n",
      "loss: 27.398865  [ 2410/ 2948]\n",
      "loss: 29.808804  [ 2510/ 2948]\n",
      "loss: 30.609823  [ 2610/ 2948]\n",
      "loss: 26.729694  [ 2710/ 2948]\n",
      "loss: 32.920902  [ 2810/ 2948]\n",
      "loss: 386.042847  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 43.732843 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 27.063686  [   10/ 2948]\n",
      "loss: 36.734608  [  110/ 2948]\n",
      "loss: 37.401726  [  210/ 2948]\n",
      "loss: 30.050745  [  310/ 2948]\n",
      "loss: 28.972782  [  410/ 2948]\n",
      "loss: 29.941120  [  510/ 2948]\n",
      "loss: 33.183468  [  610/ 2948]\n",
      "loss: 30.474934  [  710/ 2948]\n",
      "loss: 26.383509  [  810/ 2948]\n",
      "loss: 34.258553  [  910/ 2948]\n",
      "loss: 31.379725  [ 1010/ 2948]\n",
      "loss: 33.512608  [ 1110/ 2948]\n",
      "loss: 33.436440  [ 1210/ 2948]\n",
      "loss: 39.829308  [ 1310/ 2948]\n",
      "loss: 33.725040  [ 1410/ 2948]\n",
      "loss: 24.765806  [ 1510/ 2948]\n",
      "loss: 29.557688  [ 1610/ 2948]\n",
      "loss: 39.183578  [ 1710/ 2948]\n",
      "loss: 34.344536  [ 1810/ 2948]\n",
      "loss: 28.896149  [ 1910/ 2948]\n",
      "loss: 32.018463  [ 2010/ 2948]\n",
      "loss: 23.945110  [ 2110/ 2948]\n",
      "loss: 21.766220  [ 2210/ 2948]\n",
      "loss: 27.813580  [ 2310/ 2948]\n",
      "loss: 92.807411  [ 2410/ 2948]\n",
      "loss: 21.773712  [ 2510/ 2948]\n",
      "loss: 33.723137  [ 2610/ 2948]\n",
      "loss: 26.390030  [ 2710/ 2948]\n",
      "loss: 38.078831  [ 2810/ 2948]\n",
      "loss: 195.875107  [ 2910/ 2948]\n",
      "Test Error: \n",
      " Accuracy: 52.3%, Avg loss: 42.671134 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
